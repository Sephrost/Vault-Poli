### Scheduling
In modern architecture, the maximum instruction throughput is desired. thus, instruction execution overlapping is inevitable.

Modern programs are a interleaving of two kinds of operations:
- CPU operation(bursts)
- I/O operations(bursts)

![[Pasted image 20231130131311.png]]

I/O bursts are slower compared to the CPU ones. When a program is waiting for an I/O operation, it is possible to start a CPU one.

The part of a system which assigns the CPU to the different programs is the scheduler.
#### CPU scheduler
The CPU scheduler selects from among the processes in ready queue, and allocates a CPU core to one of them.

It usually implements an decision algorithm, which uses a queue during the decision phase.

CPU scheduling decisions may take place when a process generates an event, such as:
1. Switches from running to waiting state
2. Switches from running to ready state
3. Switches from waiting to ready
4. Terminates
##### Scheduler preemption
In some cases, the scheduler may decide to remove the right CPU execution to a program in favor of another one. This kind of scheduler are called preemptive

If the scheduler cannot disallocate the CPU from a process for which it has been allocated, it is called **nonpreemptive**.

> Virtually all modern operating systems including Windows, MacOS, Linux, and UNIX use preemptive scheduling algorithms.

Preemptive schedulers may cause a **race condition** when data are shared among several processes.

> Consider the case of two processes that share data. While one process is updating the data, it is preempted so that the second process can run. The second process then tries to read the data, which are in an inconsistent state.
#### Dispatcher
The dispatcher is the module that physically assign the CPU to a given process. It handles saving the state into the **Process control block**(a data structure which stores processes states with some addition informations) and restoring it when needed.

This duty involves:
- handling the context switch
- switching to user mode
- jumping to the proper location in the user program to restart that program

> Sometimes it is part of the scheduler.

A context switch requires a certain amount of time, to save the state and restore the state of another one, which is called dispatch latency.

![[Pasted image 20231130132800.png]]
#### Scheduling criteria
All scheduling algorithms are design to optimize a certain aspect of the problem.

One crucial aspect is certainly to keep the CPU as busy as possible, but other ones are also important. For example the process throughput is also a considerable factor for a processor performance.

Others criteria are:
- the **turnaround time**: the amount of time needed to completely execute a given process
- the **waiting time**: the amount of time a process has been waiting in the ready queue
- the **response time**: the amount of time it takes from when a request was submitted until the first response is produced.

Given the previously described criterias, a optimal scheduler:
- maximize the CPU utilization
- maximize the throughput
- minimize the turnaround time
- minimize the waiting time
- minimize the response time
##### FCFS: first-come first-served
Is the simplest scheduler possible.

while implementing this schema, the scheduler assigns the CPU to the first requiring process, using a FIFO queue.

The median waiting time for this algorithm is quite long.
Take into account this example:

Process|Required time|Waiting time
--|--|--
$P_1$|$24$|$0$
$P_2$|$3$|$24$
$P_3$|$3$|$27$

The average waiting time is $(0+24+27)/3=17ms$, but with the inverted order execution time would be shorter.
Thus, the the average waiting time of this schema depends on the order of the processes in the queue.

This schema is also nonpreemptive.
##### SJF: Shortest job first
This schema associates each process to the length of the next CPU burst, using it to schedule the process with the shortest time.

When the CPU is available, the scheduler assigns it to the process which has the lowest CPU burst time required. This allows to minimize the average waiting time, which makes it optimal.

Take int account the previous example, the average execution time is $(0+3+27)/3=10ms$, which is much lower.
This schema is also nonpreemptive.
##### SRTF: shortest remaining time first
The SJF schema is only theoratical, because is it difficult to estimate precisely the burst time of a process, and is also non preemptive. 

The preemptive version of the previous schema is the **shortest remaining time first** schema.

As previously said, the burst time of a process is not usually known, but can be estimated using exponential averaging:
$$τ_{n+1}=\alpha t_n+(1-\alpha)τ_n$$

where:
- $0\le \alpha \le 1$
- $t_n$ is the actual length of the $n^{th}$ CPU burst
- $τ_n$ predicts the value of the next CPU burst using the computation history

> The expanded formula is:$$τ_{n+1}=\alpha t_n+\alpha(1-\alpha)t_{n-1}+...+\alpha(1-\alpha)^jt_{n-j}+...+(1-\alpha)^{n+1}τ_0$$
##### RR: round robin
The round robin algorithm is similar to the fcfs one.
This schema divides the CPU execution time between all the waiting processes.

If there are $n$ processes in the ready queue and the time quantum is $q$, then each process gets $1/n$ of the CPU time in chunks of at most $q$ time units at once. 
No process waits more than $(n-1)q$ time units.

It also implements a timer, which is used to generate a interrupt which forces the context switch in favor of the next process.

The time quantum should be large compared to context switch time, it is usually in the order of 10 to 100 milliseconds, because the context switch usually takes less than 10 ms.
##### Priority scheduling
Another schema associates a **priority** value to each process.

The CPU is allocated to the process with the highest priority (smallest integer=highest priority).

This schema can be both preemptive or nonpreemptive.

> SJF is priority scheduling where priority is the inverse of predicted next CPU burst time.

In some cases, some processes with low priority value may never be executed. this problem is called **starvation**. To solve this, an **aging mechanism** is implemented, which increases a process priority as time passes, to ensure it is executes at some point.