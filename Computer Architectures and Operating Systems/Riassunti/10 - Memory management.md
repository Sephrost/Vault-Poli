Programs must first be loaded to the main memory to be run.

> Remember that registers and main memory are the only storage that the CPU can access directly.
>
>While register access is fast(1 clock cycle), main memory access is slower(2 or more), causing a stall.

Faster memory are preferable, but expensive, to balance cost with speed, a memory hierarchy is used.

The general memory hierarchy can be resumed by the following picture:
![[Pasted image 20240102152620.png|650]]
## Memory Protection
The main memory is divided in two portion:
- a part reserved for the resident operating system, located in the low memory portion, with the interrupt vector
- a part reserved for the user processes

In any given moment, its likely that more than one process in running on the system. 
To ensure that a process can access only the addresses that is allowed to, we limit its accessible memory area with a **base** and **limit register**, to define its logical address space.
Thus, CPU must check every memory access to ensure that is allowed(in between the base and the limit). The instructions to load the base and limit are actually privileged.

![[Pasted image 20240102152739.png|550]]
## Logical and physical addresses
To abstract the physical memory management, two kinds of addresses are used:
- **logical addresses**: generated by the CPU
- **physical addresses**: the actually addresses seen by the memory unit

Logical an physical are the same at compile and load time, and only differ from one another at runtime, mapping an address of one kind the another.

The mapping and the lookups are managed by the **memory-management unit**, which is an hardware component.
## Static partitioning
Static partitioning divides the main memory into a set of non-overlapping regions called **partition**. A partition configuration is done at compile time and cannot change over time.

> The size of a partition can be fixed, or not.

Partitioning can cause **internal fragmentation**, which is the waste of unused memory assigned to a program.

Memory assignment of partition is actually pretty easy: if there's a available segment in the memory it is assigned. If all are occupied, one is swapped out to make space for the new one.

> Because all partitions are of equal size, it does not matter which partition is used.
## Variable Partitioning
Static partitioning has some issues, for example it limits the number of the program that can run at any moment.

A more memory-efficient approach is to allow partitions to have **variable size**, only allocating the necessary memory to them.
When a process have to be allocated in the main memory, it is placed in a block of available memory large enough to accommodate it, called **hole**.
When it terminates, the partition is freed and combined with the adjacent free ones.

The OS must keep track of:
- the allocated partitions
- the holes
### Allocation problem with dynamic storage
We have previously said that a process is placed in a free hole, but how can it be chosen.
There are several approaches:
- **First-fit**: Allocate the first hole that is big enough 
- **Best-fit**: Allocate the smallest hole that is big enough. It requires to search entire list, unless ordered by size, and produces the smallest leftover hole.
- **Worst-fit**: Allocate the largest hole, It must also search entire list.

First-fit and best-fit better than worst-fit in terms of speed and storage utilization
#### Buddy system
The buddy system as a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as possible. 
It does this by using a power-of-2 allocation scheme, where holes are sized as powers of 2.

The algorithm first looks for a hole of a suitable size for the request. If it finds one, it allocates that hole to the process. 
If not, it tries to make a suitable memory hole by splitting a free memory hole larger than the requested memory size in half.
If the lower limit is reached, then the algorithm allocates that amount of memory.
This process is repeated until a suitable memory hole is found.

When a process terminates, the algorithm first frees the hole. 
Then, it looks at the neighbouring hole.
If it is free, the algorithm combines the two holes to form a bigger one. This process is repeated until either the upper limit is reached (all memory is freed), or until a non-free neighbour hole is encountered
### External fragmentation
After allocation some holes, there could be still more available memory that is not contiguous, and as such cannot be allocated directly. This problem is called **external fragmentation**.

> First fit analysis reveals that given N blocks allocated, $0.5\cdot N$ blocks lost to fragmentation.

External fragmentation can be reduced by compaction techniques.
Compaction is a process of moving memory blocks around to create a single large block of free memory. This can only be done if relocation is dynamic, meaning that processes can be moved to different memory locations at any time.

![[Pasted image 20240102162012.png|550]]
## Paging
Paging is a memory management scheme that divides physical memory into fixed-size blocks (frames) and logical memory into blocks of the same size (pages). 
When a process is executed, its pages are taken from auxiliary memory and loaded into available frames.
Each address generated by the CPU is divided into two parts: 
- a page number (p), which serves as an index for the page table containing the base address in physical memory for each page.
  - a page offset (d), which, when combined with the address specified by the page number, defines the physical memory address.
  
 The page number is used to index a page table, which contains the base address of each page in physical memory. The page offset is added to the base address to form the physical memory address.

![[Pasted image 20240102165912.png|650]]
### Solve latency of paging lookups
The page table is kept in the main memory, and in particular two registers are used to access it:
- the **page-table base register** (PTBR), which points to the page table
- the **page-table length register** (PTLR), which indicates size of the page table

In this scheme every data/instruction access requires two memory accesses, one for the page table and one for the actual data. This requires way too much time.

To solve the latency of the access, a cache can be used, called **translation lookaside buffer**.
Some TLBs store address-space identifiers (ASIDs) in each TLB entry, which are uniquely identifies each process to provide address-space protection for that process.

![[Pasted image 20240102173533.png|650]]
### Memory protection of pages
Memory protection implemented by associating protection bit with each frame to indicate if read-only or read-write access is allowed.

This is called a **validity bit**, which is attached to each entry in the page table.

> Any violations result in a trap to the kernel.

![[Pasted image 20240102174557.png|650]]
### Size of the page table
Memory structures for paging can get huge using straightforward methods.
Consider a 32-bit logical address space on modern computers, with a page size of 4 KB($2^{12}$). The page table would have 1 million entries($2^{32}/2^{12}$), and if each entry is 4 bytes, this means that each process would require 4 MB of physical address space for the page table alone. 
This is not ideal, as we do not want to allocate that much contiguous memory in main memory.

One simple solution to this problem is to divide the page table into smaller units. This can be done using hierarchical paging, hashed page tables, or inverted page tables.
#### Hierarchical page tables
In hierarchical paging, the page table is divided into multiple levels.
Each level of the page table points to the next level down, until the final level points to the actual physical memory frames. This reduces the amount of contiguous memory required for the page table, as each level of the page table can be stored in a different part of main memory.

![[Pasted image 20240102180220.png|550]]

Now lets consider the case of a two level paging.
A logical address (on 32-bit machine with 4K page size) is divided into: 
- a page number consisting of 20 bits 
- a page offset consisting of 12 bits

Since the page table is paged, the page number is further divided into: 
- a 10-bit page number 
- a 10-bit page offset

![[Pasted image 20240102180516.png|550]]
where $p_1$ is an index into the outer page table, and $p_2$ is the displacement within the page of the inner page table.
#### Hashed page tables
In hashed page tables, the page table entries are stored in a hash table. 
This allows the page table entries to be scattered throughout main memory, rather than having to be stored contiguously.
This can further reduce the amount of contiguous memory required for the page table.

> This approach is common in 32 bit address spaces.

![[Pasted image 20240102181508.png|650]]
#### Inverted Page Table
In inverted page tables, each physical memory frame has a list of all the virtual addresses that map to it. 
This is the opposite of a traditional page table, where each virtual address has an entry in the page table that points to the corresponding physical memory frame. 
Inverted page tables can further reduce the amount of contiguous memory required for the page table, but they are more complex to implement and manage.

## Memory management in real-time systems
Memory management techniques must not introduce nondeterministic delays.

For real-time 
- Static partitioning shall be used 
- Dynamic memory shall not be used 
- Memory compaction shall not be used 
### Stack sharing
In conventional OS, each process must have a private stack space, to store its content and variables.
But this could require large amounts of memory.

If all tasks are independent they can share a single stack space.

When a task $\tau$ is preempted by a task $\tau'$
- $\tau$ maintains its stack 
- the stack of $\tau′$ is allocated immediately above that of $t$

Since task $\tau′$ cannot be blocked by $\tau$, its stack can never be penetrated by that of $\tau$, which can resume only after $\tau′$ is completed.