#### Cache mapping
A key design decision is where blocks (or lines) can be placed in a cache. The most popular scheme is **set associative**, where a set is a **group** of blocks in the cache.

A block is first mapped onto a **set**, and then the block can be placed *anywhere* within that set.

The set is chosen by the address of the data:
$$(block\; address)\mod (number\; of \; cache\; set)$$
Another way to address the problem of cache placement is allow any memory location to be stored in any position of the cache. A cache that implements this approach is called *fully associative*, which is a particular case of set associative cache, where there's only one set withing one cache.

An opposite way to address the problem is also a particular case of set associativity, where one memory location can be mapped only to one location of the cache. This approach is called *direct mapping*, in which the size of a block is equal to one.

![[Pasted image 20231102132133.png|550]]
#### Replacing algorithm
Now that we have explained how to place and retrieve the content of a memory location in the cache, we should talk about how a cache replaces it's blocks.

There are many possible approaches:
- LRU (Least Recently Used): the most used, not according to the teacher.
- FIFO (First-In First-Out): the cheapest
- LFU (Least Frequently Used): theoretically, the most effective, but really hard to implement in hardware in a way that requires a single cycle of clock.
- random: simple and effective.
###### Pseudo LRU algorithm
pLRU is an efficient approximation of a LRU algorithm.
The age of the cache ways is arranged in a binary tree, where eache node represents a *history bit*.

#### Updating the main memory
The consistency between the content of the cache and the correspondent address in the main memory is always required.

As such, two main strategies are proposed.
##### Write-back
For each cache block, a flag (called *dirty bit*) is introduced, which remembers whether or not the block has been changed since it was loaded into the cache.

When a block is evicted from the cache and the *dirty bit is set*, the **block** is **copied** from the cache to the main memory.

Such strategy has a few disadvantages:
- the replacement is slower because it sometimes requires copying in the main memory the replaced block
- in multiprocessor systems there may be incoherence between the caches of different processors
- it may not be possible to restore memory data after possible system failures
###### Write-through
**Each time** the CPU performs a write operation, it **writes on both** the cache data and the main memory data.

The resulting loss of efficiency is limited by the fact that writing operations are usually much less numerous than reading ones.